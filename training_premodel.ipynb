{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--batch_size N] [--num_classes N]\n",
      "                   [--test_batch_size N] [--epochs N] [--start_epoch N]\n",
      "                   [--resume PATH] [--lr LR] [--momentum M] [--weight_decay W]\n",
      "                   [--no_cuda] [--seed S] [--log_inteval N] [--arch ARCH]\n",
      "                   [--train_log TRAIN_LOG] [--val_log VAL_LOG]\n",
      "                   [--checkpoint_path CHECKPOINT_PATH]\n",
      "__main__.py: error: unrecognized arguments: -f /run/user/1014/jupyter/kernel-3b10c2ec-1aa3-476d-b169-a9c1eef178f9.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import shutil\n",
    "import torch, cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.utils.data as Data\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from logger import Logger\n",
    "from preprocessingdata import myImageFloder\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "\n",
    "\n",
    "try:\n",
    "    from StringIO import StringIO      # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO             # Python 3.x\n",
    "\n",
    "class Logger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    \n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "        # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format='png')\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                               height=img.shape[0],\n",
    "                               width=img.shape[1])\n",
    "\n",
    "            # Create a Summary value\n",
    "        img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag,i),image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "        \n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "        \n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "        \n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "        \n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()\n",
    "\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Pytorch Scene Recognition Training')\n",
    "parser.add_argument('--batch_size', type=int, default=100, metavar='N',\n",
    "                   help='input batch size for training(default:100)')\n",
    "parser.add_argument('--num_classes', type=int, default=80, metavar='N',\n",
    "                    help='the total classes kinds number of scene classifier')\n",
    "parser.add_argument('--test_batch_size', type=int, default=100, metavar='N',\n",
    "                   help='input batch size for testing')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                   help='number of epochs to train')\n",
    "parser.add_argument('--start_epoch', type=int, default=0, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--resume', type=str, default='', metavar='PATH',\n",
    "                    help='path to latest checkpoint (default:none)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                   help='initial learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                   help='SGD momentum')\n",
    "parser.add_argument('--weight_decay', '--wd', type=float, default=1e-4, metavar='W',\n",
    "                   help='wight decay(default: 1e-4)')\n",
    "parser.add_argument('--no_cuda', action='store_true', default=False,\n",
    "                   help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                   help='random seed(default:1)')\n",
    "parser.add_argument('--log_inteval', type=int, default=50, metavar='N',\n",
    "                   help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--arch', '-a', default='alexnet', metavar='ARCH',\n",
    "                   help='the deep model architecture (default: alexnet)')\n",
    "\n",
    "parser.add_argument('--train_log', type=str, default='/home/haoyanlong/AI/logs/alexnet_log/train',\n",
    "                    help='the train parameters logging')\n",
    "parser.add_argument('--val_log', type=str, default='/home/haoyanlong/AI/logs/alexnet_log/val',\n",
    "                    help='the validation curracy logging')\n",
    "parser.add_argument('--checkpoint_path', type=str, default='/home/haoyanlong/AI/modelcheckpoint/alexnet/',\n",
    "                    help='the directory of model checkpoint saved')\n",
    "best_prec1 = 0\n",
    "\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        # Because the CrossEntropyLoss() is calculated averagely on mini-batch\n",
    "        # size_average (bool, optional)â€“By default, the losses are averaged over observations for each minibatch\n",
    "        # so, val * n means the total loss on the mini-batch\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Sets the learning rate to the initial LR decayed by 10 every 30epochs\n",
    "    :param optimizer:the optimizer class\n",
    "    :param epoch:int the training epoch\n",
    "    :return:the lr\n",
    "    \"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_groups in optimizer.param_groups:\n",
    "        param_groups['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, label, topk=(1,3,5)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = label.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = torch.eq(pred, label.view(1,-1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    :param state: the model state\n",
    "    :param is_best: the current whether is the best\n",
    "    :param filename: the name of filename saved\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    torch.save(state, filename + '_lastest.pth.tar')\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename + '_lastest.pth.tar', filename + '_best.path.tar')\n",
    "\n",
    "\n",
    "def finemodel(predmodel):\n",
    "    \"\"\"\n",
    "    :param predmodel: the model pretrained on places365\n",
    "    :return: the model fine changed\n",
    "    \"\"\"\n",
    "    model = torch.load(predmodel)\n",
    "\n",
    "    for i, params in enumerate(model.parameters()):\n",
    "        params.requires_grad = False\n",
    "\n",
    "    # The new classifier removes the last layer\n",
    "    new_classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    # the size of last output features: 4096\n",
    "    num_features = model.classifier[-1].in_features\n",
    "\n",
    "    new_classifier.add_module('scene_classify', nn.Linear(num_features, args.num_classes))\n",
    "\n",
    "    model.classifier = new_classifier\n",
    "    return model\n",
    "\n",
    "\n",
    "# fine-tuning the predmodel\n",
    "def train(train_dataset, predmodel, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    :param predmodel: (model) fine-changed model\n",
    "    :param epoch: the current training epoch\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top3 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # logger event\n",
    "    logger = Logger(args.train_log)\n",
    "\n",
    "    # switch to train mode\n",
    "    predmodel.train()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for batch_idx, (image, label) in enumerate(train_dataset,1):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if not args.no_cuda and torch.cuda.is_available():\n",
    "            image, label = image.cuda(), label.cuda()\n",
    "\n",
    "        image, label = Variable(image), Variable(label)\n",
    "\n",
    "        # compute output\n",
    "        output = predmodel(image)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec3, prec5 = accuracy(output=output.data, label=label, topk=(1,3,5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1, input.size(0))\n",
    "        top3.update(prec3, input.size(0))\n",
    "        top5.update(prec5, input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if batch_idx % args.log_interval == 99:\n",
    "            print ('Train Epoch: [{0}], Train processing:[{1} / {2}\\t'\n",
    "                   'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                   'Data {data_time.val:.3f} ({data_time.avg: .3f})\\t'\n",
    "                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                   'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                   'Prec@3 {top3.val:.3f} ({top3.avg:.3f})\\t'\n",
    "                   'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\\t'.format(\n",
    "                    epoch, batch_idx, len(train_dataset),\n",
    "                    batch_time=batch_time,\n",
    "                    data_time=data_time,\n",
    "                    loss=losses,\n",
    "                    top1=top1,top3=top3,top5=top5)\n",
    "            )\n",
    "\n",
    "            ################# tensorboard logging ######################\n",
    "            step = batch_idx * (epoch + 1)\n",
    "            # (1) Log the scalar values\n",
    "            info = {\n",
    "                'loss_val': losses.val,\n",
    "                'loss_avg': losses.avg,\n",
    "                'top1_val': top1.val,\n",
    "                'top1_avg': top1.avg,\n",
    "                'top3_val': top3.val,\n",
    "                'top3_avg': top3.avg,\n",
    "                'top5_val': top5.val,\n",
    "                'top5_avg': top5.avg\n",
    "                }\n",
    "            for tag, value in info.items():\n",
    "                logger.scalar_summary(tag, value, step)\n",
    "            # (2) Log values and gradients of the parameter (histogram)\n",
    "            for tag, value in predmodel.named_parameters():\n",
    "                tag = tag.replace('.','/')\n",
    "                logger.histo_summary(tag, value.data.cpu().numpy(), step)\n",
    "                logger.histo_summary(tag+'/grad', value.grad.cpu().numpy(), step)\n",
    "\n",
    "            # (3) Log the images\n",
    "            info = {\n",
    "                'images': image.view(-1,3,224,224).cpu().numpy()[:10]\n",
    "                }\n",
    "            for tag, image in info.items():\n",
    "                logger.image_summary(tag, image, step)\n",
    "\n",
    "\n",
    "# Validation the finedmodel\n",
    "def validate(validation_dataset, predmodel, criterion):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top3 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # logger event\n",
    "    logger = Logger(args.val_log)\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    predmodel.eval()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for idx, (image, label) in enumerate(validation_dataset):\n",
    "        if not args.no_cuda and torch.cuda.is_available():\n",
    "            image, label = image.cuda(), label.cuda()\n",
    "\n",
    "        image, label = Variable(image, volatile=True), Variable(label, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        output = predmodel(image)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec3, prec5 = accuracy(output=output.data, label=label, topk=(1, 3, 5))\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1, input.size(0))\n",
    "        top3.update(prec3, input.size(0))\n",
    "        top5.update(prec5, input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if idx % args.log_interval == 0:\n",
    "            print ('Test: [{0}/{1}]\\t'\n",
    "                   'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                   'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                   'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "                   'Prec@3 {top3.val:.3f} ({top3.avg:.3f})\\t'\n",
    "                   'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "                    idx,len(validation_dataset),\n",
    "                    batch_time=batch_time,\n",
    "                    loss = losses,\n",
    "                    top1=top1,\n",
    "                    top3=top3,\n",
    "                    top5=top5))\n",
    "\n",
    "            ################# tensorboard logging ######################\n",
    "            # (1) Log the scalar values\n",
    "            info = {\n",
    "                'loss_val': losses.val,\n",
    "                'loss_avg': losses.avg,\n",
    "                'top1_val': top1.val,\n",
    "                'top1_avg': top1.avg,\n",
    "                'top3_val': top3.val,\n",
    "                'top3_avg': top3.avg,\n",
    "                'top5_val': top5.val,\n",
    "                'top5_avg': top5.avg\n",
    "            }\n",
    "            for tag, value in info.items():\n",
    "                logger.scalar_summary(tag, value, idx)\n",
    "            # (2) Log values and gradients of the parameter (histogram)\n",
    "            for tag, value in predmodel.named_parameters():\n",
    "                tag = tag.replace('.', '/')\n",
    "                logger.histo_summary(tag, value.data.cpu().numpy(), idx)\n",
    "                logger.histo_summary(tag + '/grad', value.grad.cpu().numpy(), idx)\n",
    "\n",
    "            # (3) Log the images\n",
    "            info = {\n",
    "                'images': image.view(-1, 3, 224, 224).cpu().numpy()[:10]\n",
    "            }\n",
    "            for tag, image in info.items():\n",
    "                logger.image_summary(tag, image, idx)\n",
    "    # return top1.avg, top3.avg, top5.avg\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    global args, best_prec1\n",
    "    args = parser.parse_args()\n",
    "    print args\n",
    "    \n",
    "    \"\"\"\n",
    "    # Preparing data\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "    train_dataloader = myImageFloder(\n",
    "        root='/home1/haoyanlong/AiChallenger_datasets/scenerecognition/ai_challenger_scene_train_20170904/scene_train_images_20170904',\n",
    "        label='/home1/haoyanlong/AiChallenger_datasets/scenerecognition/ai_challenger_scene_train_20170904/train_data.txt',\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.RandomSizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize]\n",
    "        ))\n",
    "    train_data = Data.DataLoader(dataset=train_dataloader,\n",
    "                             batch_size=args.batch_size,\n",
    "                             shuffle=True,\n",
    "                             num_workers=2)\n",
    "\n",
    "\n",
    "    val_dataloader = myImageFloder(\n",
    "        root='/home1/haoyanlong/AiChallenger_datasets/scenerecognition/ai_challenger_scene_validation_20170908/scene_validation_images_20170908',\n",
    "        label='/home1/haoyanlong/AiChallenger_datasets/scenerecognition/ai_challenger_scene_validation_20170908/validation_data.txt',\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Scale(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize]\n",
    "        ))\n",
    "    val_data = Data.DataLoader(dataset=val_dataloader,\n",
    "                               batch_size=args.batch_size,\n",
    "                               shuffle=False,\n",
    "                               num_workers=2)\n",
    "\n",
    "\n",
    "    # create model\n",
    "    print \"=> creating model\"\n",
    "    model = finemodel(\"/home/haoyanlong/AI/place365_premodel/whole_{}_places365.pth.tar\".format(args.arch))\n",
    "    print model\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print (\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            print (\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print (\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = optim.SGD(model.scene_classify.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          momentum=args.momentum,\n",
    "                          weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer=optimizer,\n",
    "                             epoch=epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_datset=train_data, predmodel=model, criterion=criterion, optimizer=optimizer, epoch=epoch)\n",
    "\n",
    "        # evalute on validation set\n",
    "        prec1 = validation(validation_dataset=val_data, predmodel=model, criterion=criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.arch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1},\n",
    "            is_best=is_best,\n",
    "            filename=args.checkpoint_path + args.arch.lower())\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--batch_size N] [--num_classes N]\n",
      "                   [--test_batch_size N] [--epochs N] [--start_epoch N]\n",
      "                   [--resume PATH] [--lr LR] [--momentum M] [--weight_decay W]\n",
      "                   [--no_cuda] [--seed S] [--log_inteval N] [--arch ARCH]\n",
      "                   [--train_log TRAIN_LOG] [--val_log VAL_LOG]\n",
      "                   [--checkpoint_path CHECKPOINT_PATH]\n",
      "__main__.py: error: unrecognized arguments: -f /run/user/1014/jupyter/kernel-f055fe4f-d241-4a00-b68b-b4075a771b19.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/common/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Pytorch Scene Recognition Training')\n",
    "parser.add_argument('--batch_size', type=int, default=100, metavar='N',\n",
    "                   help='input batch size for training(default:100)')\n",
    "parser.add_argument('--num_classes', type=int, default=80, metavar='N',\n",
    "                    help='the total classes kinds number of scene classifier')\n",
    "parser.add_argument('--test_batch_size', type=int, default=100, metavar='N',\n",
    "                   help='input batch size for testing')\n",
    "parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                   help='number of epochs to train')\n",
    "parser.add_argument('--start_epoch', type=int, default=0, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--resume', type=str, default='', metavar='PATH',\n",
    "                    help='path to latest checkpoint (default:none)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                   help='initial learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                   help='SGD momentum')\n",
    "parser.add_argument('--weight_decay', '--wd', type=float, default=1e-4, metavar='W',\n",
    "                   help='wight decay(default: 1e-4)')\n",
    "parser.add_argument('--no_cuda', action='store_true', default=False,\n",
    "                   help='disables CUDA training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                   help='random seed(default:1)')\n",
    "parser.add_argument('--log_inteval', type=int, default=50, metavar='N',\n",
    "                   help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--arch', '-a', default='alexnet', metavar='ARCH',\n",
    "                   help='the deep model architecture (default: alexnet)')\n",
    "\n",
    "parser.add_argument('--train_log', type=str, default='/home/haoyanlong/AI/logs/alexnet_log/train',\n",
    "                    help='the train parameters logging')\n",
    "parser.add_argument('--val_log', type=str, default='/home/haoyanlong/AI/logs/alexnet_log/val',\n",
    "                    help='the validation curracy logging')\n",
    "parser.add_argument('--checkpoint_path', type=str, default='/home/haoyanlong/AI/modelcheckpoint/alexnet/',\n",
    "                    help='the directory of model checkpoint saved')\n",
    "\n",
    "def main():\n",
    "\n",
    "    global args, best_prec1\n",
    "    args = parser.parse_args()\n",
    "    print args\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-440002edb1e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-440002edb1e9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_prec1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/common/anaconda2/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/common/anaconda2/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2372\u001b[0m         \"\"\"\n\u001b[1;32m   2373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2374\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s: error: %s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/common/anaconda2/lib/python2.7/argparse.pyc\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
